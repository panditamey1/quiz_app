Learning Objective,Question,Option A (Correct),Option B (Incorrect),Option C (Incorrect),Feedback A,Feedback B,Feedback C
Explain the significance of eigenvalues and eigenvectors in Principal Component Analysis (PCA),What role do eigenvalues play in PCA?,They indicate the variance captured by each principal component,They determine the clustering of the dataset,Eigenvalues define the number of principal components to be used,"Eigenvalues measure how much variance is captured by each principal component, which helps in determining their importance.",Eigenvalues do not relate directly to clustering but to the amount of variance each principal component captures from the data.,"The number of principal components is determined by the data analyst or through criteria like the explained variance ratio, not directly by eigenvalues alone."
Explain the significance of eigenvalues and eigenvectors in Principal Component Analysis (PCA),What does an eigenvector in PCA represent?,It represents a direction in the feature space along which data variability is maximal,It represents the mean vector of the dataset,It defines the outliers in the dataset,"In PCA, eigenvectors define directions of maximum variance in the data, and they form the new axes upon which data is projected.","The mean vector of the dataset is not represented by any eigenvector in PCA; instead, it is usually subtracted from the dataset before PCA is applied.",Eigenvectors do not define outliers; they represent axes of maximum variance in the dataset.
Describe different types of optimizers in machine learning and highlight how the Adam optimizer differs from RMSprop,What common feature do both Adam and RMSprop optimizers include?,Both use the concept of adaptive learning rates for each parameter,Both use fixed learning rates throughout training,Both adjust learning rates based on the batch size,"Both Adam and RMSprop optimizers adapt the learning rates based on the average of the second moments of the gradients, making them suited for tasks with sparse gradients.","Fixed learning rates are not a feature of either Adam or RMSprop, both of which adapt learning rates based on past gradient magnitudes.",Learning rates in Adam and RMSprop are not adjusted directly based on batch size but based on the calculated moments of gradients.
Describe different types of optimizers in machine learning and highlight how the Adam optimizer differs from RMSprop,How does the Adam optimizer differ from RMSprop in terms of adjusting learning rates?,Adam combines momentum with RMSprop's adaptive learning rates,RMSprop uses momentum while Adam does not,"Adam adjusts learning rates based on past gradients only, unlike RMSprop","Adam uses both the first and second moment estimates to adjust the learning rate for each parameter, which incorporates elements of momentum, enhancing its efficiency over RMSprop.","Unlike RMSprop, which only adjusts learning rates based on the second moment (variance) of the gradients, Adam also incorporates momentum.","Adam incorporates both first (momentum) and second moment estimates (variance of gradients), unlike RMSprop which primarily focuses on the second moment."
Identify different types of activation functions and explain the vanishing gradient problem in neural networks,Which activation function is known for exacerbating the vanishing gradient problem?,Sigmoid,ReLU,Leaky ReLU,"The sigmoid function outputs values between 0 and 1, which can cause gradients to vanish during backpropagation, especially in deep networks.","ReLU, while popular for not saturating in the positive domain, does not typically cause vanishing gradients; it can lead to dying ReLU problem where neurons output zeros due to negative inputs.",Leaky ReLU is designed to address the vanishing gradient problem by allowing a small gradient when the unit is inactive and does not exacerbate it.
