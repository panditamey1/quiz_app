Learning Objective,Question,Option A (Correct),Option B (Incorrect),Option C (Incorrect),Feedback A,Feedback B,Feedback C
Describe strategies for handling data imbalance issues in model training,Why is under-sampling not always the best choice for dealing with highly imbalanced datasets?,It can discard potentially useful information contained within the majority class,It creates an exact balance between the classes,It increases the computational load,Discarding data from the majority class can lead to loss of important information and potentially bias the training process.,"Under-sampling does not create balance; it reduces the size of the majority class to match the minority class, which could still result in a balanced or imbalanced dataset.",Under-sampling generally reduces computational demands as it decreases the dataset size by reducing the number of majority class samples.
"Compare and contrast gradient descent and stochastic gradient descent, including preferences under different scenarios",What is the primary difference between gradient descent and stochastic gradient descent?,"Gradient descent uses all data points to update parameters once per iteration, while stochastic gradient descent updates parameters more frequently using a subset of data",Stochastic gradient descent requires more computational resources,Stochastic gradient descent calculates parameters less accurately,"Gradient descent updates the model's parameters less frequently per epoch, leading to potentially smoother convergence but slower iteration on large datasets.",Stochastic gradient descent actually uses less memory and computational resources as it processes smaller data batches.,"While stochastic gradient descent may be less precise per update due to the smaller data sample, it generally does not calculate parameters less accurately overall."
"Compare and contrast gradient descent and stochastic gradient descent, including preferences under different scenarios",Which gradient descent method is generally faster per iteration?,Stochastic gradient descent,Gradient descent,Both methods are equally fast,"Stochastic gradient descent updates parameters after computing the gradient on a small subset of data, making it faster per iteration.","Gradient descent uses all available data for each update, which can be computationally expensive and slow on very large datasets.","The speed of convergence depends on the data and problem specifics, but in terms of iteration speed, stochastic gradient descent is faster due to smaller data processing batches."
Understand logistic regression and derive the gradient descent algorithm for it,What is the key feature of logistic regression that makes it suitable for binary classification problems?,"The output is modeled as a probability using the logistic function, which bounds it between 0 and 1",It can directly handle multi-class classification problems,It uses the least squares error as its loss function,"Logistic regression models the probability of the default class (for example, class 1) through the logistic function, which is an S-shaped curve.",Logistic regression typically handles binary classification; extensions like multinomial logistic are used for multi-class problems.,"Logistic regression does not use least squares error, which is typically used in linear regression. Instead, it uses a logistic loss function."
Understand logistic regression and derive the gradient descent algorithm for it,"In the context of logistic regression, what does the gradient descent algorithm minimize?","The cost function, which is typically the logistic loss function",The likelihood function,The model's complexity parameter,"The logistic loss function quantifies the difference between the predicted probabilities and the actual class values, and gradient descent works to minimize this error.","The likelihood function is maximized in logistic regression, not minimized. It relates to the fit of the model to the data, whereas gradient descent minimizes the loss.","Minimizing complexity is not the direct aim of gradient descent in logistic regression; rather, it's about optimizing the specific loss function."
