Learning Objective,Question,Option A (Correct),Option B (Incorrect),Option C (Incorrect),Feedback A,Feedback B,Feedback C
Understand the concept of overfitting and regularization in machine learning,What typically causes overfitting in a machine learning model?,Having too many parameters relative to the number of observations,Too few parameters in the model,Using a simpler model,"Overfitting occurs when a model is too complex, having many parameters which make it fit the noise rather than the underlying pattern of the data.","A model with too few parameters is more likely to underfit than overfit, as it cannot capture the complexity of the data.","Using a simpler model actually helps in reducing overfitting, not causing it."
Understand the concept of overfitting and regularization in machine learning,Which technique is commonly used to prevent overfitting?,Regularization,Increasing model complexity,Removing all penalties from the model,"Regularization techniques, such as L1 and L2, add a penalty on the size of the coefficients to prevent the model from becoming overly complex.","Increasing model complexity is more likely to lead to overfitting, not prevent it.","Removing penalties would typically increase the risk of overfitting, as the model parameters are less constrained."
Explain the bias-variance tradeoff and its implications in model training,What does the bias-variance tradeoff imply for machine learning models?,A tradeoff between a modelâ€™s ability to minimize assumptions about the data (bias) and its sensitivity to data variability (variance),That increasing bias always reduces variance,High variance is preferable to high bias,Bias refers to errors introduced by approximating a real-world problem which may not satisfy any mathematical model. Variance is an error from sensitivity to small fluctuations in the training set. The tradeoff is managing these to minimize the total error.,"While reducing variance can sometimes increase bias, it is not always true that increasing bias reduces variance.",Neither high variance nor high bias is universally preferable; the goal is to find a balance that minimizes total error.
Explain the bias-variance tradeoff and its implications in model training,"In the context of bias-variance tradeoff, what would likely result from a model with high bias?",The model is overly simplistic and cannot capture complex patterns in the data,The model is overly complex and overfits the training data,The model perfectly fits both training and testing datasets,"A high-bias model makes strong assumptions about the data and tends to be simplistic, missing the relevant relations between features and target outputs.","A high-bias model is not typically complex but rather too simple, which cannot adequately capture the complexity of the data.","An overfitting model will not fit the testing dataset well, hence this statement is incorrect for high-bias scenarios."
Describe strategies for handling data imbalance issues in model training,What is a common method to address data imbalance in classification problems?,Using techniques like SMOTE to synthetically generate minority class examples,Removing all instances of the majority class,Using a larger test dataset,"SMOTE (Synthetic Minority Over-sampling Technique) helps to create artificial data based on the existing minority data, thus balancing the class distribution.",Simply removing majority class instances can lead to loss of valuable information and is generally not a recommended approach.,"While using a larger test dataset may help in evaluating the model better, it does not address the underlying issue of class imbalance during training."
